步骤 2：写一个“从样本池构建训练集”的 loader

逻辑：

从 stock_pool_ml_samples 按时间区间取样本

join 上：

stock_bollinger_data（所有布林特征）

stock_tick_daily_features（tick 聚合特征）

得到一个 df：

包含：ts_code, trade_date, y_ret_10d + 所有数值特征

自动选出特征列（所有数值列 - 标签）

步骤 3：特征清洗（非常简单的一层）

对所有特征：

做 1% / 99% 分位数剪裁（防极端值炸掉模型）

NaN 填 0（或者中位数）

步骤 4：时间切分 + 训练 XGBoost

按 trade_date 排序

用配置里的 train_end / valid_end 切分 train / valid / test

用 XGBRegressor 训练回归模型（预测 y_ret_10d）

步骤 5：评估 + 保存模型

在 test 区间上做：

每日截面 Spearman IC

分组回测（按预测值分 5 组，看平均真实 y_ret_10d）

保存模型 + feature_cols 等元信息到一个 pkl（比如 models/xgb_from_pool_ret10d.pkl）

二、核心代码 1：样本池数据加载器

文件建议：ml/pool_data_loader.py

# ml/pool_data_loader.py

from __future__ import annotations

import datetime as dt
from dataclasses import dataclass
from typing import List, Tuple

import numpy as np
import pandas as pd
from sqlalchemy import create_engine
from sqlalchemy.engine import Engine


@dataclass
class DBConfig:
    db_url: str = "postgresql+psycopg2://user:password@localhost:5432/quant"


def get_engine(cfg: DBConfig) -> Engine:
    return create_engine(cfg.db_url)


@dataclass
class PoolDataRangeConfig:
    start_date: dt.date
    end_date: dt.date
    label_col: str = "y_ret_10d"


def load_pool_merged_dataset(
    engine: Engine,
    dr: PoolDataRangeConfig,
) -> Tuple[pd.DataFrame, List[str], str]:
    """
    从 stock_pool_ml_samples 出发，join 上
      - stock_bollinger_data
      - stock_tick_daily_features
    得到训练用的 df + feature_cols + label_col。
    """

    sql = f"""
    SELECT
        s.ts_code,
        s.trade_date::date AS trade_date,
        s.{dr.label_col}    AS {dr.label_col},
        b.*,
        t.*
    FROM stock_pool_ml_samples s
    JOIN stock_bollinger_data b
      ON s.ts_code = b.ts_code
     AND s.trade_date = b.trade_date
    LEFT JOIN stock_tick_daily_features t
      ON s.ts_code = t.ts_code
     AND s.trade_date = t.trade_date
    WHERE s.trade_date >= :start_date
      AND s.trade_date <= :end_date
      AND s.{dr.label_col} IS NOT NULL
    ORDER BY s.ts_code, s.trade_date;
    """

    df = pd.read_sql(
        sql,
        engine,
        params={"start_date": dr.start_date, "end_date": dr.end_date},
        parse_dates=["trade_date"],
    )
    if df.empty:
        raise RuntimeError("No samples found in stock_pool_ml_samples for given range.")

    # 去重列名（ts_code/trade_date/y_ret_10d 在 b 中可能重复）
    df = df.loc[:, ~df.columns.duplicated()]

    df["trade_date"] = df["trade_date"].dt.date

    # 只保留数值列
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if dr.label_col not in numeric_cols:
        raise RuntimeError(f"Label column {dr.label_col} not found in numeric columns.")

    feature_cols = [c for c in numeric_cols if c != dr.label_col]

    return df, feature_cols, dr.label_col


def clean_features(df: pd.DataFrame, feature_cols: List[str]) -> pd.DataFrame:
    """
    简单特征清洗：
      - 对每个特征做 1%/99% quantile clip
      - 用 0 填 NaN（你也可以改成中位数）
    """
    df = df.copy()
    for col in feature_cols:
        col_series = df[col].astype(float)
        # 可能有全 NaN 的列，跳过
        if col_series.notna().sum() == 0:
            df[col] = 0.0
            continue

        q1, q99 = col_series.quantile([0.01, 0.99])
        df[col] = col_series.clip(q1, q99)
        df[col] = df[col].fillna(0.0)

    return df


if __name__ == "__main__":
    db_cfg = DBConfig()
    engine = get_engine(db_cfg)

    # 样本从 2024-11-15 开始，这里举例取到今天
    dr = PoolDataRangeConfig(
        start_date=dt.date(2024, 11, 15),
        end_date=dt.date.today(),
        label_col="y_ret_10d",
    )

    df, feat_cols, label_col = load_pool_merged_dataset(engine, dr)
    df = clean_features(df, feat_cols)

    print(df.head())
    print("Feature num:", len(feat_cols), "Label:", label_col)

三、核心代码 2：基于样本池训练 XGBoost

文件建议：ml/train_xgb_from_pool.py

# ml/train_xgb_from_pool.py

from __future__ import annotations

import datetime as dt
from dataclasses import dataclass
from typing import List, Tuple

import joblib
import numpy as np
import pandas as pd
from scipy.stats import spearmanr
from xgboost import XGBRegressor

from ml.pool_data_loader import (
    DBConfig,
    PoolDataRangeConfig,
    get_engine,
    load_pool_merged_dataset,
    clean_features,
)


@dataclass
class TrainConfig:
    model_path: str = "models/xgb_from_pool_ret10d.pkl"

    # 训练/验证/测试时间切分
    # 注意：必须落在样本的时间范围内（>= 2024-11-15）
    train_end: dt.date = dt.date(2025, 1, 31)
    valid_end: dt.date = dt.date(2025, 2, 28)

    # XGBoost 参数
    n_estimators: int = 600
    max_depth: int = 6
    learning_rate: float = 0.05
    subsample: float = 0.8
    colsample_bytree: float = 0.8
    reg_lambda: float = 1.0
    reg_alpha: float = 0.0


def time_split(
    df: pd.DataFrame,
    label_col: str,
    feature_cols: List[str],
    cfg: TrainConfig,
):
    df = df.sort_values("trade_date").reset_index(drop=True)

    train_mask = df["trade_date"] <= cfg.train_end
    valid_mask = (df["trade_date"] > cfg.train_end) & (df["trade_date"] <= cfg.valid_end)
    test_mask = df["trade_date"] > cfg.valid_end

    X_train = df.loc[train_mask, feature_cols].to_numpy(dtype=float)
    y_train = df.loc[train_mask, label_col].to_numpy(dtype=float)

    X_valid = df.loc[valid_mask, feature_cols].to_numpy(dtype=float)
    y_valid = df.loc[valid_mask, label_col].to_numpy(dtype=float)

    X_test = df.loc[test_mask, feature_cols].to_numpy(dtype=float)
    y_test = df.loc[test_mask, label_col].to_numpy(dtype=float)

    print(
        f"Train size: {X_train.shape}, "
        f"Valid size: {X_valid.shape}, "
        f"Test size: {X_test.shape}"
    )
    return (X_train, y_train), (X_valid, y_valid), (X_test, y_test)


def calc_daily_ic(df: pd.DataFrame, pred_col: str, label_col: str) -> pd.Series:
    """
    按 trade_date 做截面 Spearman IC。
    """
    ics = []
    dates = []
    for d, sub in df.groupby("trade_date"):
        if sub[pred_col].nunique() <= 1:
            continue
        ic, _ = spearmanr(sub[pred_col], sub[label_col])
        ics.append(ic)
        dates.append(d)
    return pd.Series(ics, index=dates, name="ic")


def quantile_backtest(
    df: pd.DataFrame,
    pred_col: str,
    label_col: str,
    n_quantiles: int = 5,
) -> pd.DataFrame:
    """
    简单分组回测：每天按预测值分成 n 组，计算每组平均真实收益。
    """
    records = []
    for d, sub in df.groupby("trade_date"):
        if len(sub) < n_quantiles:
            continue
        try:
            sub = sub.copy()
            sub["q"] = pd.qcut(sub[pred_col], q=n_quantiles, labels=False) + 1
        except ValueError:
            continue

        for q, sub_q in sub.groupby("q"):
            records.append(
                {"trade_date": d, "quantile": int(q), "avg_ret": sub_q[label_col].mean()}
            )

    df_q = pd.DataFrame(records)
    if df_q.empty:
        return df_q

    pivot = df_q.pivot(index="trade_date", columns="quantile", values="avg_ret").sort_index()
    pivot.columns = [f"Q{int(c)}" for c in pivot.columns]
    return pivot


def train_from_pool():
    # 1. 连接数据库
    db_cfg = DBConfig()
    engine = get_engine(db_cfg)

    # 2. 选择样本时间范围（至少覆盖 train_end / valid_end / test）
    data_range = PoolDataRangeConfig(
        start_date=dt.date(2024, 11, 15),
        end_date=dt.date.today(),
        label_col="y_ret_10d",
    )

    # 3. 载入样本池数据 + 特征
    df, feature_cols, label_col = load_pool_merged_dataset(engine, data_range)
    print(f"Raw merged df shape: {df.shape}, feature num: {len(feature_cols)}")

    # 4. 特征清洗
    df = clean_features(df, feature_cols)

    # 5. 时间切分
    cfg = TrainConfig(
        model_path="models/xgb_from_pool_ret10d.pkl",
        # 这里的日期你需要根据当前已有样本的最新日期来设
        train_end=dt.date(2025, 1, 31),
        valid_end=dt.date(2025, 2, 28),
    )
    (X_train, y_train), (X_valid, y_valid), (X_test, y_test) = time_split(
        df, label_col, feature_cols, cfg
    )

    # 6. 训练 XGBoost
    model = XGBRegressor(
        n_estimators=cfg.n_estimators,
        max_depth=cfg.max_depth,
        learning_rate=cfg.learning_rate,
        subsample=cfg.subsample,
        colsample_bytree=cfg.colsample_bytree,
        reg_lambda=cfg.reg_lambda,
        reg_alpha=cfg.reg_alpha,
        objective="reg:squarederror",
        n_jobs=8,
    )

    model.fit(
        X_train,
        y_train,
        eval_set=[(X_valid, y_valid)],
        verbose=50,
    )

    # 7. 评估（主要看 test 段）
    df_eval = df.copy()
    X_all = df_eval[feature_cols].to_numpy(dtype=float)
    df_eval["pred_ret_10d"] = model.predict(X_all)

    df_test = df_eval[df_eval["trade_date"] > cfg.valid_end].copy()
    if not df_test.empty:
        ic_series = calc_daily_ic(df_test, "pred_ret_10d", label_col)
        print("Test IC mean:", ic_series.mean(), "std:", ic_series.std())

        q_ret = quantile_backtest(df_test, "pred_ret_10d", label_col, n_quantiles=5)
        print("Average return by quantile (test period):")
        print(q_ret.mean())

    # 8. 保存模型
    bundle = {
        "model": model,
        "feature_cols": feature_cols,
        "label_col": label_col,
        "train_end": cfg.train_end,
        "valid_end": cfg.valid_end,
    }
    joblib.dump(bundle, cfg.model_path)
    print(f"Model saved to {cfg.model_path}")


if __name__ == "__main__":
    train_from_pool()