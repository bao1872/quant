一、开发步骤（按文件改）

只动一个文件：ml/pool_data_loader.py

做三件事：

在文件顶部增加一个 关键时序特征列表 KEY_TS_FEATURES。

新增函数 add_lag1_and_delta_features，按 ts_code 分组计算 lag1 和 delta1。

在 load_pool_merged_dataset 里，在 concat 完所有 chunk 后调用这个函数，再选数值特征列。

⚠️ KEY_TS_FEATURES 里的名字要和你表里的列名对上，不对的列函数会自动跳过，我这里先给一个推荐列表，你可以按自己实际字段改。

二、修改代码示例（贴进去就能用）

下面是基于你当前仓库结构写的 核心代码补丁。
你可以直接在 ml/pool_data_loader.py 里按位置修改。

1. 顶部增加关键特征列表

在 from db.connection import get_engine 下面，加上：

from db.connection import get_engine

# ========= 关键时序特征（只对这些列做 lag1 + delta1） =========
# 请根据你实际的字段名调整：
# - price_position / v_price_position / band_width_zscore / v_band_width_zscore 是 stock_bollinger_data 里的
# - active_buy_vol_ratio 假设是你在 tick 聚合表里算好的“主买占比”
KEY_TS_FEATURES: List[str] = [
    "price_position",
    "v_price_position",
    "band_width_zscore",
    "v_band_width_zscore",
    # 如果你的 tick 日表里叫别的名字，比如 main_buy_vol_ratio，就把这行改成真实列名
    "active_buy_vol_ratio",
]
# ============================================================


如果暂时不确定 tick 那个字段叫什么，可以先只保留前 4 个布林特征，把 active_buy_vol_ratio 先注释掉。

2. 在文件中间新增：add_lag1_and_delta_features

建议放在 load_pool_merged_dataset 定义前面，比如紧跟在 _date_chunks 或 load_pool_merged_dataset_iter 之后。

def add_lag1_and_delta_features(
    df: pd.DataFrame,
    key_features: List[str],
) -> pd.DataFrame:
    """
    只对少数关键特征增加简单时间序列信息：
      对每个特征 col in key_features：
        col_lag1   = 前 1 个交易日的值（按 ts_code 分组）
        col_delta1 = col - col_lag1  （当天相对昨天的变化）

    要求 df 至少包含：
      - ts_code
      - trade_date
      - key_features 中的部分列（缺的会自动跳过）
    """
    if df.empty:
        return df

    # 先按 ts_code, trade_date 排序，保证 shift/rolling 顺序正确
    df = df.sort_values(["ts_code", "trade_date"]).copy()

    grouped = df.groupby("ts_code", sort=False)

    for col in key_features:
        if col not in df.columns:
            # 某些特征可能在当前样本期内缺失，这里直接跳过
            print(f"[lag] skip feature={col}, not in df.columns", flush=True)
            continue

        lag_col = f"{col}_lag1"
        delta_col = f"{col}_delta1"

        # t-1
        df[lag_col] = grouped[col].shift(1)

        # 当天相对昨天变化
        df[delta_col] = df[col] - df[lag_col]

    return df

3. 修改 load_pool_merged_dataset：插入调用

你当前的 load_pool_merged_dataset 大致是这样（我从你仓库里读出来的）：

def load_pool_merged_dataset(dr: PoolDataRangeConfig) -> Tuple[pd.DataFrame, List[str], str]:
    frames = []
    total_rows = 0
    for df in load_pool_merged_dataset_iter(dr, chunk_days=30):
        frames.append(df)
        total_rows += len(df)
    df = pd.concat(frames, ignore_index=True)
    if dr.label_col in df.columns:
        df[dr.label_col] = pd.to_numeric(df[dr.label_col], errors="coerce")
    print("[load_pool] concat rows=", total_rows, flush=True)
    if df.empty:
        raise RuntimeError("No samples found in stock_pool_ml_samples for given range.")
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if dr.label_col not in numeric_cols:
        raise RuntimeError("Label column not found in numeric columns.")
    feature_cols = [c for c in numeric_cols if c != dr.label_col and not c.startswith("y_")]
    return df, feature_cols, dr.label_col


请改成下面这版（只改这一处）：

def load_pool_merged_dataset(dr: PoolDataRangeConfig) -> Tuple[pd.DataFrame, List[str], str]:
    frames = []
    total_rows = 0
    for df_chunk in load_pool_merged_dataset_iter(dr, chunk_days=30):
        frames.append(df_chunk)
        total_rows += len(df_chunk)

    df = pd.concat(frames, ignore_index=True)
    print("[load_pool] concat rows=", total_rows, flush=True)

    if df.empty:
        raise RuntimeError("No samples found in stock_pool_ml_samples for given range.")

    # label 转成数值
    if dr.label_col in df.columns:
        df[dr.label_col] = pd.to_numeric(df[dr.label_col], errors="coerce")

    # ===== 在这里增加：关键特征的 lag1 + delta1 =====
    df = add_lag1_and_delta_features(df, KEY_TS_FEATURES)
    # =================================================

    # 选数值列
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if dr.label_col not in numeric_cols:
        raise RuntimeError("Label column not found in numeric columns.")

    # 特征列 = 所有数值列 - 标签列 - 其他 y_* 标签
    feature_cols = [
        c for c in numeric_cols
        if c != dr.label_col and not c.startswith("y_")
    ]

    return df, feature_cols, dr.label_col


这样做的顺序是：

所有 chunk 合并 → df

转 label 类型

增加 lag1/delta1 这几个新列

然后统一选数值列作为特征

三、训练脚本可以保持现状（用 A 的配置）

你的 ml/train_xgb_from_pool.py 里，A 实验那套配置已经验证过效果不错：

单日特征

label clipping（1%/99%）

当前 XGB 参数

在刚才这套修改完成后：

feature_cols 会多出少量 *_lag1 和 *_delta1 列（数量很有限）；

训练脚本逻辑不需要改，只要重新 python -m ml.train_xgb_from_pool 跑一遍即可。

建议你暂时先继续使用：

单日 + lag1 + delta1

label clipping（你 log 里那组 q1/q99 区间就可以）

四、你接下来可以这样操作

按上面的代码片段，修改 ml/pool_data_loader.py：

加 KEY_TS_FEATURES；

加 add_lag1_and_delta_features；

在 load_pool_merged_dataset 中插入调用。

确认 KEY_TS_FEATURES 里所有列在 df.columns 里都存在；

如果某列不存在，会打印 [lag] skip feature=xxx，不影响运行。

跑训练：

cd d:\quant
python -m ml.train_xgb_from_pool